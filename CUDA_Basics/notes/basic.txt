kernel_name <<< numOfBlocks, threadsPerBlock >>>(arguments)

https://www.youtube.com/playlist?list=PLTgRMOcmRb3O5Xc8PJckYdbyCr5HPGx4e

threads are grouped into blocks
and blocks are grouped into grids

blocks and grids can have 1, 2, or 3 dimensions
threads within the same blocks can share resources and synchronize with each other 

Hardware architecture
- streaming multiproecessors (SMs)
	- each SMs contain a single cuda processor
	- also cache, registers and memory
	- also has global memory shared by all the SMs
	- cuda cores dont have branch predictions or speculative execution

- Single Instruction, Multiple Threads (SIMT) architecture
	- unlike SIMD, the vector width is variable
	- threads have independent states
	- cuda executes threads in a group of 32 threads (called Warps)
		- all the threads in a Warp will run at the same time
		- execution context stays in the SM so no overhead for switching between threads

- Hardware multitherading support

https://www.youtube.com/playlist?list=PLxNPSjHT5qvtYRVdNN1yDcdSl39uHV_sU
blocks
- can be in 3D
	- helpful for mapping the problem

grids
- how a problem is mapped to the gpu
- part of the GPU launch parameters
- can be 3D

