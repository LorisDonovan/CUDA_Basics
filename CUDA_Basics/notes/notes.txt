Unified Memory
- single memory for both host and device memory

cudaMallocManaged()

prefetch:
cudaMemPrefetchAsync(x, bytes, id);
- to improve performance
_____________________________________________________________________________________________________

Matrix Multiplication
- for every element of the result matrix we assign a new thread

blockIdx and threadIdx
blockDim
- no of threads in the block in x or y or z direction

map the matrix in to a 2D block

row = blockIdx.x * blockDim.y + threadIdx.x
col = blockIdx.y * blockDim.x + threadIdx.y

performance
- coalecing writes
	- 2d to 1d
- shared memory

cache tiling 
- DRAM is slow
	- so caching is needed
	- use shared memory
		- user managed L1 cache
		- private per thread block
		- but entire input won't fit in the cache so we only put pieces of input in the cache

__shared__ = to initialize shared memory
__shared__ creates a copy of the variable for each block
		- every thread in tht block shares this memory
		- threads cannot see or modify shared memory of other blocks
		- but synchronization of threads is necessary 
__syncthreads() guarantees that every thread in the block has completed its execution prior to it
				- syncs threads in a block
				- dont use it in places where only some threads can execute it (ie, in if() statement)
reduction: producing a smallar array after operating on the input arrays
// if we iterate over the smallar array with one thread the time complexity will linearly increase
// we can reduce the time complexity to logarithmic 


Coalescing
- second matrix is col major so elements are far apart
	- transpose the first matrix
- make both matrix col maj



Constant Memory
- in GPU, the bottleneck is not the throughput of arithmetic units rather the memory bandwidth
- constant memory stores data tht will not change over the course of kernel execution
- using constant memory rather than global memory will reduce the required memory bandwidth


